[2024-02-04T09:13:54.474+0100] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jeniferhdfs.submit_spark_job scheduled__2024-02-03T00:00:00+00:00 [queued]>
[2024-02-04T09:13:54.488+0100] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jeniferhdfs.submit_spark_job scheduled__2024-02-03T00:00:00+00:00 [queued]>
[2024-02-04T09:13:54.489+0100] {taskinstance.py:2171} INFO - Starting attempt 1 of 6
[2024-02-04T09:13:54.528+0100] {taskinstance.py:2192} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-02-03 00:00:00+00:00
[2024-02-04T09:13:54.537+0100] {standard_task_runner.py:60} INFO - Started process 111498 to run task
[2024-02-04T09:13:54.544+0100] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'jeniferhdfs', 'submit_spark_job', 'scheduled__2024-02-03T00:00:00+00:00', '--job-id', '148', '--raw', '--subdir', 'DAGS_FOLDER/hdfsDag.py', '--cfg-path', '/tmp/tmpik5_pzs2']
[2024-02-04T09:13:54.548+0100] {standard_task_runner.py:88} INFO - Job 148: Subtask submit_spark_job
[2024-02-04T09:13:54.634+0100] {task_command.py:423} INFO - Running <TaskInstance: jeniferhdfs.submit_spark_job scheduled__2024-02-03T00:00:00+00:00 [running]> on host ubuntu.ubuntu.virtualbox.org
[2024-02-04T09:13:54.750+0100] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='jenifer' AIRFLOW_CTX_DAG_ID='jeniferhdfs' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-02-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-02-03T00:00:00+00:00'
[2024-02-04T09:13:54.759+0100] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-02-04T09:13:54.762+0100] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark --class main.scala.mnm.MnMcount --queue root.default --deploy-mode client /home/ubuntu/airflow/mnmcount/scala/target/scala-2.12/main-scala-mnm_2.12-1.0.jar
[2024-02-04T09:14:10.106+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:10 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2024-02-04T09:14:10.117+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-02-04T09:14:17.567+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:17 INFO SparkContext: Running Spark version 3.3.4
[2024-02-04T09:14:18.263+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-02-04T09:14:18.812+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceUtils: ==============================================================
[2024-02-04T09:14:18.817+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-02-04T09:14:18.819+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceUtils: ==============================================================
[2024-02-04T09:14:18.822+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO SparkContext: Submitted application: MnMCount
[2024-02-04T09:14:18.911+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-02-04T09:14:18.946+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceProfile: Limiting resource is cpu
[2024-02-04T09:14:18.948+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-02-04T09:14:19.137+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:19 INFO SecurityManager: Changing view acls to: ubuntu
[2024-02-04T09:14:19.140+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:19 INFO SecurityManager: Changing modify acls to: ubuntu
[2024-02-04T09:14:19.142+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:19 INFO SecurityManager: Changing view acls groups to:
[2024-02-04T09:14:19.145+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:19 INFO SecurityManager: Changing modify acls groups to:
[2024-02-04T09:14:19.146+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
[2024-02-04T09:14:20.948+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:20 INFO Utils: Successfully started service 'sparkDriver' on port 33743.
[2024-02-04T09:14:21.218+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO SparkEnv: Registering MapOutputTracker
[2024-02-04T09:14:21.618+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO SparkEnv: Registering BlockManagerMaster
[2024-02-04T09:14:21.705+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-02-04T09:14:21.708+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-02-04T09:14:21.726+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-02-04T09:14:21.902+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0de46b50-37bb-4aa0-9477-df8c02cadcfd
[2024-02-04T09:14:21.964+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-02-04T09:14:22.012+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-02-04T09:14:22.992+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-02-04T09:14:22.995+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2024-02-04T09:14:23.022+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2024-02-04T09:14:23.111+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO SparkContext: Added JAR file:/home/ubuntu/airflow/mnmcount/scala/target/scala-2.12/main-scala-mnm_2.12-1.0.jar at spark://10.0.2.15:33743/jars/main-scala-mnm_2.12-1.0.jar with timestamp 1707034457457
[2024-02-04T09:14:23.524+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO Executor: Starting executor ID driver on host 10.0.2.15
[2024-02-04T09:14:23.555+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-02-04T09:14:23.601+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO Executor: Fetching spark://10.0.2.15:33743/jars/main-scala-mnm_2.12-1.0.jar with timestamp 1707034457457
[2024-02-04T09:14:23.943+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:33743 after 201 ms (0 ms spent in bootstraps)
[2024-02-04T09:14:23.973+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:23 INFO Utils: Fetching spark://10.0.2.15:33743/jars/main-scala-mnm_2.12-1.0.jar to /tmp/spark-accce164-802d-49ea-aa51-f4fe34771d1c/userFiles-81adb69b-06e1-4916-9ffb-acdcaa60a1a8/fetchFileTemp10006034557133941389.tmp
[2024-02-04T09:14:24.280+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO Executor: Adding file:/tmp/spark-accce164-802d-49ea-aa51-f4fe34771d1c/userFiles-81adb69b-06e1-4916-9ffb-acdcaa60a1a8/main-scala-mnm_2.12-1.0.jar to class loader
[2024-02-04T09:14:24.307+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33381.
[2024-02-04T09:14:24.308+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO NettyBlockTransferService: Server created on 10.0.2.15:33381
[2024-02-04T09:14:24.316+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-02-04T09:14:24.349+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 33381, None)
[2024-02-04T09:14:24.495+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:33381 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 33381, None)
[2024-02-04T09:14:24.545+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 33381, None)
[2024-02-04T09:14:24.551+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 33381, None)
[2024-02-04T09:14:29.000+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-02-04T09:14:29.069+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:29 INFO SharedState: Warehouse path is 'file:/home/ubuntu/spark-warehouse'.
[2024-02-04T09:14:34.696+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:34 INFO InMemoryFileIndex: It took 416 ms to list leaf files for 1 paths.
[2024-02-04T09:14:35.593+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
[2024-02-04T09:14:35.842+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2024-02-04T09:14:35.851+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:33381 (size: 34.2 KiB, free: 434.4 MiB)
[2024-02-04T09:14:35.862+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:35 INFO SparkContext: Created broadcast 0 from json at MnMcount.scala:30
[2024-02-04T09:14:37.188+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO FileInputFormat: Total input files to process : 1
[2024-02-04T09:14:37.197+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO FileInputFormat: Total input files to process : 1
[2024-02-04T09:14:37.331+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO SparkContext: Starting job: json at MnMcount.scala:30
[2024-02-04T09:14:37.392+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Got job 0 (json at MnMcount.scala:30) with 1 output partitions
[2024-02-04T09:14:37.396+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at MnMcount.scala:30)
[2024-02-04T09:14:37.398+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Parents of final stage: List()
[2024-02-04T09:14:37.403+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:14:37.420+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at MnMcount.scala:30), which has no missing parents
[2024-02-04T09:14:37.662+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KiB, free 434.2 MiB)
[2024-02-04T09:14:37.672+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.2 MiB)
[2024-02-04T09:14:37.677+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:33381 (size: 4.8 KiB, free: 434.4 MiB)
[2024-02-04T09:14:37.682+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:14:37.735+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at MnMcount.scala:30) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:14:37.753+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-02-04T09:14:38.015+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, ANY, 4628 bytes) taskResourceAssignments Map()
[2024-02-04T09:14:38.116+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-02-04T09:14:38.488+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:38 INFO BinaryFileRDD: Input split: Paths:/user/project/datalake/raw/2024-02-04/tmdb_popular_movies.json:0+6636236
[2024-02-04T09:14:39.721+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2357 bytes result sent to driver
[2024-02-04T09:14:39.748+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1791 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:14:39.758+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-02-04T09:14:39.790+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO DAGScheduler: ResultStage 0 (json at MnMcount.scala:30) finished in 2.260 s
[2024-02-04T09:14:39.838+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:14:39.841+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-02-04T09:14:39.855+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:39 INFO DAGScheduler: Job 0 finished: json at MnMcount.scala:30, took 2.520402 s
[2024-02-04T09:14:45.132+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:33381 in memory (size: 4.8 KiB, free: 434.4 MiB)
[2024-02-04T09:14:46.882+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:33381 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2024-02-04T09:14:52.977+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:52 INFO FileSourceStrategy: Pushed Filters:
[2024-02-04T09:14:52.980+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(year(cast(gettimestamp(release_date#9, yyyy-MM-dd, TimestampType, Some(Europe/Paris), false) as date)))
[2024-02-04T09:14:52.985+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:52 INFO FileSourceStrategy: Output Data Schema: struct<id: bigint, release_date: string>
[2024-02-04T09:14:54.893+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:14:55.611+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:14:55.615+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:14:55.926+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:14:55.957+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:14:56.054+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:14:56.141+0100] {spark_submit.py:571} INFO - 24/02/04 09:14:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:00.853+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO CodeGenerator: Code generated in 1315.045032 ms
[2024-02-04T09:15:00.874+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.4 KiB, free 434.2 MiB)
[2024-02-04T09:15:00.925+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 434.2 MiB)
[2024-02-04T09:15:00.931+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:33381 (size: 34.1 KiB, free: 434.4 MiB)
[2024-02-04T09:15:00.940+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO SparkContext: Created broadcast 2 from parquet at MnMcount.scala:47
[2024-02-04T09:15:00.969+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-04T09:15:01.315+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Registering RDD 6 (parquet at MnMcount.scala:47) as input to shuffle 0
[2024-02-04T09:15:01.333+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Got map stage job 1 (parquet at MnMcount.scala:47) with 1 output partitions
[2024-02-04T09:15:01.334+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (parquet at MnMcount.scala:47)
[2024-02-04T09:15:01.335+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Parents of final stage: List()
[2024-02-04T09:15:01.337+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:01.347+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at parquet at MnMcount.scala:47), which has no missing parents
[2024-02-04T09:15:01.449+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 41.6 KiB, free 434.1 MiB)
[2024-02-04T09:15:01.455+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.1 MiB)
[2024-02-04T09:15:01.460+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:33381 (size: 19.6 KiB, free: 434.3 MiB)
[2024-02-04T09:15:01.462+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:01.466+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at parquet at MnMcount.scala:47) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:01.467+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-02-04T09:15:01.491+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, ANY, 4943 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:01.493+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-02-04T09:15:01.742+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO CodeGenerator: Code generated in 24.936555 ms
[2024-02-04T09:15:01.863+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO CodeGenerator: Code generated in 19.267497 ms
[2024-02-04T09:15:01.911+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO CodeGenerator: Code generated in 12.098789 ms
[2024-02-04T09:15:01.936+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO CodeGenerator: Code generated in 15.900094 ms
[2024-02-04T09:15:01.953+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/user/project/datalake/raw/2024-02-04/tmdb_popular_movies.json, range: 0-6636236, partition values: [empty row]
[2024-02-04T09:15:01.972+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:01 INFO CodeGenerator: Code generated in 13.902754 ms
[2024-02-04T09:15:03.063+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2823 bytes result sent to driver
[2024-02-04T09:15:03.071+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1598 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:03.072+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-02-04T09:15:03.077+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: ShuffleMapStage 1 (parquet at MnMcount.scala:47) finished in 1.715 s
[2024-02-04T09:15:03.078+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: looking for newly runnable stages
[2024-02-04T09:15:03.079+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: running: Set()
[2024-02-04T09:15:03.081+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: waiting: Set()
[2024-02-04T09:15:03.082+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: failed: Set()
[2024-02-04T09:15:03.139+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-02-04T09:15:03.216+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:33381 in memory (size: 19.6 KiB, free: 434.4 MiB)
[2024-02-04T09:15:03.382+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-02-04T09:15:03.443+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO CodeGenerator: Code generated in 46.359836 ms
[2024-02-04T09:15:03.550+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO CodeGenerator: Code generated in 42.775501 ms
[2024-02-04T09:15:03.623+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO SparkContext: Starting job: parquet at MnMcount.scala:47
[2024-02-04T09:15:03.641+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Got job 2 (parquet at MnMcount.scala:47) with 1 output partitions
[2024-02-04T09:15:03.642+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at MnMcount.scala:47)
[2024-02-04T09:15:03.643+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2024-02-04T09:15:03.645+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:03.653+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at parquet at MnMcount.scala:47), which has no missing parents
[2024-02-04T09:15:03.679+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 41.0 KiB, free 434.1 MiB)
[2024-02-04T09:15:03.682+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.1 MiB)
[2024-02-04T09:15:03.684+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:33381 (size: 19.6 KiB, free: 434.3 MiB)
[2024-02-04T09:15:03.686+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:03.689+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at parquet at MnMcount.scala:47) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:03.690+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-02-04T09:15:03.699+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:03.710+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2024-02-04T09:15:03.837+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO ShuffleBlockFetcherIterator: Getting 1 (5.9 KiB) non-empty blocks including 1 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:03.842+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms
[2024-02-04T09:15:03.914+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 7686 bytes result sent to driver
[2024-02-04T09:15:03.924+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 229 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:03.927+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: ResultStage 3 (parquet at MnMcount.scala:47) finished in 0.253 s
[2024-02-04T09:15:03.928+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:15:03.930+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-02-04T09:15:03.933+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-02-04T09:15:03.935+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Job 2 finished: parquet at MnMcount.scala:47, took 0.308575 s
[2024-02-04T09:15:03.952+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Registering RDD 12 (parquet at MnMcount.scala:47) as input to shuffle 1
[2024-02-04T09:15:03.953+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Got map stage job 3 (parquet at MnMcount.scala:47) with 1 output partitions
[2024-02-04T09:15:03.954+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at MnMcount.scala:47)
[2024-02-04T09:15:03.954+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2024-02-04T09:15:03.955+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:03.963+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[12] at parquet at MnMcount.scala:47), which has no missing parents
[2024-02-04T09:15:03.986+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 44.9 KiB, free 434.1 MiB)
[2024-02-04T09:15:03.991+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 434.0 MiB)
[2024-02-04T09:15:03.993+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:33381 (size: 20.5 KiB, free: 434.3 MiB)
[2024-02-04T09:15:03.995+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:03.997+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[12] at parquet at MnMcount.scala:47) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:03.998+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-02-04T09:15:04.001+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:04.016+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)
[2024-02-04T09:15:04.081+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ShuffleBlockFetcherIterator: Getting 1 (5.9 KiB) non-empty blocks including 1 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:04.082+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-02-04T09:15:04.339+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 4138 bytes result sent to driver
[2024-02-04T09:15:04.344+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 345 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:04.346+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-02-04T09:15:04.348+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: ShuffleMapStage 5 (parquet at MnMcount.scala:47) finished in 0.383 s
[2024-02-04T09:15:04.349+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: looking for newly runnable stages
[2024-02-04T09:15:04.350+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: running: Set()
[2024-02-04T09:15:04.351+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: waiting: Set()
[2024-02-04T09:15:04.352+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: failed: Set()
[2024-02-04T09:15:04.361+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-02-04T09:15:04.415+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO CodeGenerator: Code generated in 23.703537 ms
[2024-02-04T09:15:04.460+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO SparkContext: Starting job: parquet at MnMcount.scala:47
[2024-02-04T09:15:04.462+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Got job 4 (parquet at MnMcount.scala:47) with 1 output partitions
[2024-02-04T09:15:04.463+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at MnMcount.scala:47)
[2024-02-04T09:15:04.464+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2024-02-04T09:15:04.465+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:04.469+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[14] at parquet at MnMcount.scala:47), which has no missing parents
[2024-02-04T09:15:04.503+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 238.6 KiB, free 433.8 MiB)
[2024-02-04T09:15:04.508+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 88.8 KiB, free 433.7 MiB)
[2024-02-04T09:15:04.509+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:33381 (size: 88.8 KiB, free: 434.2 MiB)
[2024-02-04T09:15:04.511+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:04.512+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[14] at parquet at MnMcount.scala:47) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:04.513+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-02-04T09:15:04.516+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:04.518+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
[2024-02-04T09:15:04.573+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ShuffleBlockFetcherIterator: Getting 1 (7.1 KiB) non-empty blocks including 1 (7.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:04.574+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-02-04T09:15:04.621+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO CodeGenerator: Code generated in 13.330295 ms
[2024-02-04T09:15:04.649+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:04.650+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:04.651+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:04.652+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:04.653+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:04.655+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:04.664+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO CodecConfig: Compression: SNAPPY
[2024-02-04T09:15:04.668+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO CodecConfig: Compression: SNAPPY
[2024-02-04T09:15:04.752+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ParquetOutputFormat: Parquet block size to 134217728
[2024-02-04T09:15:04.753+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ParquetOutputFormat: Validation is off
[2024-02-04T09:15:04.755+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2024-02-04T09:15:04.756+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ParquetOutputFormat: Parquet properties are:
[2024-02-04T09:15:04.757+0100] {spark_submit.py:571} INFO - Parquet page size to 1048576
[2024-02-04T09:15:04.758+0100] {spark_submit.py:571} INFO - Parquet dictionary page size to 1048576
[2024-02-04T09:15:04.759+0100] {spark_submit.py:571} INFO - Dictionary is true
[2024-02-04T09:15:04.760+0100] {spark_submit.py:571} INFO - Writer version is: PARQUET_1_0
[2024-02-04T09:15:04.761+0100] {spark_submit.py:571} INFO - Page size checking is: estimated
[2024-02-04T09:15:04.762+0100] {spark_submit.py:571} INFO - Min row count for page size check is: 100
[2024-02-04T09:15:04.763+0100] {spark_submit.py:571} INFO - Max row count for page size check is: 10000
[2024-02-04T09:15:04.772+0100] {spark_submit.py:571} INFO - Truncate length for column indexes is: 64
[2024-02-04T09:15:04.774+0100] {spark_submit.py:571} INFO - Truncate length for statistics min/max  is: 2147483647
[2024-02-04T09:15:04.776+0100] {spark_submit.py:571} INFO - Bloom filter enabled: false
[2024-02-04T09:15:04.779+0100] {spark_submit.py:571} INFO - Max Bloom filter size for a column is 1048576
[2024-02-04T09:15:04.779+0100] {spark_submit.py:571} INFO - Bloom filter expected number of distinct values are: null
[2024-02-04T09:15:04.780+0100] {spark_submit.py:571} INFO - Page row count limit to 20000
[2024-02-04T09:15:04.780+0100] {spark_submit.py:571} INFO - Writing page checksums is: on
[2024-02-04T09:15:04.893+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:33381 in memory (size: 19.6 KiB, free: 434.3 MiB)
[2024-02-04T09:15:04.971+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:33381 in memory (size: 20.5 KiB, free: 434.3 MiB)
[2024-02-04T09:15:04.997+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-02-04T09:15:04.999+0100] {spark_submit.py:571} INFO - {
[2024-02-04T09:15:05.001+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-04T09:15:05.004+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-04T09:15:05.005+0100] {spark_submit.py:571} INFO - "name" : "year",
[2024-02-04T09:15:05.006+0100] {spark_submit.py:571} INFO - "type" : "integer",
[2024-02-04T09:15:05.008+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-04T09:15:05.009+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:05.009+0100] {spark_submit.py:571} INFO - }, {
[2024-02-04T09:15:05.010+0100] {spark_submit.py:571} INFO - "name" : "total_movies",
[2024-02-04T09:15:05.011+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-04T09:15:05.012+0100] {spark_submit.py:571} INFO - "nullable" : false,
[2024-02-04T09:15:05.012+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:05.012+0100] {spark_submit.py:571} INFO - } ]
[2024-02-04T09:15:05.012+0100] {spark_submit.py:571} INFO - }
[2024-02-04T09:15:05.013+0100] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-02-04T09:15:05.013+0100] {spark_submit.py:571} INFO - message spark_schema {
[2024-02-04T09:15:05.013+0100] {spark_submit.py:571} INFO - optional int32 year;
[2024-02-04T09:15:05.013+0100] {spark_submit.py:571} INFO - required int64 total_movies;
[2024-02-04T09:15:05.013+0100] {spark_submit.py:571} INFO - }
[2024-02-04T09:15:05.014+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:05.014+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:05.190+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:05 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-02-04T09:15:07.207+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileOutputCommitter: Saved output of task 'attempt_20240204091504357102744612304669_0008_m_000000_4' to hdfs://localhost:9000/user/project/datalake/usage/2024-02-04/movies_progression_years.parquet/_temporary/0/task_20240204091504357102744612304669_0008_m_000000
[2024-02-04T09:15:07.208+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO SparkHadoopMapRedUtil: attempt_20240204091504357102744612304669_0008_m_000000_4: Committed. Elapsed time: 33 ms.
[2024-02-04T09:15:07.231+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5862 bytes result sent to driver
[2024-02-04T09:15:07.255+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 2719 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:07.265+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-02-04T09:15:07.266+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: ResultStage 8 (parquet at MnMcount.scala:47) finished in 2.774 s
[2024-02-04T09:15:07.267+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:15:07.270+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-02-04T09:15:07.271+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Job 4 finished: parquet at MnMcount.scala:47, took 2.792801 s
[2024-02-04T09:15:07.272+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileFormatWriter: Start to commit write Job 2657b3f0-ceb1-44d6-98e7-f8009b12e9b8.
[2024-02-04T09:15:07.428+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileFormatWriter: Write Job 2657b3f0-ceb1-44d6-98e7-f8009b12e9b8 committed. Elapsed time: 153 ms.
[2024-02-04T09:15:07.437+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileFormatWriter: Finished processing stats for write job 2657b3f0-ceb1-44d6-98e7-f8009b12e9b8.
[2024-02-04T09:15:07.594+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileSourceStrategy: Pushed Filters:
[2024-02-04T09:15:07.596+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(year(cast(gettimestamp(release_date#9, yyyy-MM-dd, TimestampType, Some(Europe/Paris), false) as date)))
[2024-02-04T09:15:07.598+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileSourceStrategy: Output Data Schema: struct<id: bigint, release_date: string>
[2024-02-04T09:15:07.733+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.4 KiB, free 433.7 MiB)
[2024-02-04T09:15:07.756+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.6 MiB)
[2024-02-04T09:15:07.761+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:33381 (size: 34.1 KiB, free: 434.2 MiB)
[2024-02-04T09:15:07.767+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO SparkContext: Created broadcast 7 from show at MnMcount.scala:49
[2024-02-04T09:15:07.771+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-04T09:15:07.815+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Registering RDD 18 (show at MnMcount.scala:49) as input to shuffle 2
[2024-02-04T09:15:07.816+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Got map stage job 5 (show at MnMcount.scala:49) with 1 output partitions
[2024-02-04T09:15:07.817+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (show at MnMcount.scala:49)
[2024-02-04T09:15:07.817+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Parents of final stage: List()
[2024-02-04T09:15:07.817+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:07.821+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[18] at show at MnMcount.scala:49), which has no missing parents
[2024-02-04T09:15:07.842+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 41.7 KiB, free 433.6 MiB)
[2024-02-04T09:15:07.845+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 433.6 MiB)
[2024-02-04T09:15:07.848+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.2.15:33381 (size: 19.6 KiB, free: 434.2 MiB)
[2024-02-04T09:15:07.851+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:07.853+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[18] at show at MnMcount.scala:49) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:07.853+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-02-04T09:15:07.863+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (10.0.2.15, executor driver, partition 0, ANY, 4943 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:07.864+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)
[2024-02-04T09:15:07.916+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:07 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/user/project/datalake/raw/2024-02-04/tmdb_popular_movies.json, range: 0-6636236, partition values: [empty row]
[2024-02-04T09:15:08.745+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 2780 bytes result sent to driver
[2024-02-04T09:15:08.768+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 897 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:08.768+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-02-04T09:15:08.778+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO DAGScheduler: ShuffleMapStage 9 (show at MnMcount.scala:49) finished in 0.951 s
[2024-02-04T09:15:08.779+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO DAGScheduler: looking for newly runnable stages
[2024-02-04T09:15:08.780+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO DAGScheduler: running: Set()
[2024-02-04T09:15:08.784+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO DAGScheduler: waiting: Set()
[2024-02-04T09:15:08.785+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO DAGScheduler: failed: Set()
[2024-02-04T09:15:08.880+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-02-04T09:15:08.922+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-02-04T09:15:09.362+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO SparkContext: Starting job: show at MnMcount.scala:49
[2024-02-04T09:15:09.392+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO DAGScheduler: Got job 6 (show at MnMcount.scala:49) with 1 output partitions
[2024-02-04T09:15:09.393+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO DAGScheduler: Final stage: ResultStage 11 (show at MnMcount.scala:49)
[2024-02-04T09:15:09.416+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2024-02-04T09:15:09.487+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:09.560+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.2.15:33381 in memory (size: 19.6 KiB, free: 434.2 MiB)
[2024-02-04T09:15:09.575+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[22] at show at MnMcount.scala:49), which has no missing parents
[2024-02-04T09:15:09.805+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:33381 in memory (size: 88.8 KiB, free: 434.3 MiB)
[2024-02-04T09:15:09.979+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:09 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 41.1 KiB, free 433.9 MiB)
[2024-02-04T09:15:10.041+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 433.9 MiB)
[2024-02-04T09:15:10.041+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.2.15:33381 (size: 19.6 KiB, free: 434.3 MiB)
[2024-02-04T09:15:10.072+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:10.074+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[22] at show at MnMcount.scala:49) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:10.075+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-02-04T09:15:10.096+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 6) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:10.137+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO Executor: Running task 0.0 in stage 11.0 (TID 6)
[2024-02-04T09:15:10.735+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO ShuffleBlockFetcherIterator: Getting 1 (5.9 KiB) non-empty blocks including 1 (5.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:10.736+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 117 ms
[2024-02-04T09:15:12.597+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO Executor: Finished task 0.0 in stage 11.0 (TID 6). 10421 bytes result sent to driver
[2024-02-04T09:15:12.723+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 6) in 2633 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:12.733+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO DAGScheduler: ResultStage 11 (show at MnMcount.scala:49) finished in 3.104 s
[2024-02-04T09:15:12.737+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:15:12.771+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-02-04T09:15:12.772+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-02-04T09:15:12.772+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO DAGScheduler: Job 6 finished: show at MnMcount.scala:49, took 3.416279 s
[2024-02-04T09:15:12.829+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:12 INFO CodeGenerator: Code generated in 20.147111 ms
[2024-02-04T09:15:13.137+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:13 INFO CodeGenerator: Code generated in 199.106916 ms
[2024-02-04T09:15:13.177+0100] {spark_submit.py:571} INFO - +----+------------+
[2024-02-04T09:15:13.178+0100] {spark_submit.py:571} INFO - |year|total_movies|
[2024-02-04T09:15:13.178+0100] {spark_submit.py:571} INFO - +----+------------+
[2024-02-04T09:15:13.179+0100] {spark_submit.py:571} INFO - |1902|           1|
[2024-02-04T09:15:13.179+0100] {spark_submit.py:571} INFO - |1920|           1|
[2024-02-04T09:15:13.180+0100] {spark_submit.py:571} INFO - |1921|           1|
[2024-02-04T09:15:13.180+0100] {spark_submit.py:571} INFO - |1922|           1|
[2024-02-04T09:15:13.181+0100] {spark_submit.py:571} INFO - |1923|           1|
[2024-02-04T09:15:13.181+0100] {spark_submit.py:571} INFO - |1924|           1|
[2024-02-04T09:15:13.181+0100] {spark_submit.py:571} INFO - |1925|           2|
[2024-02-04T09:15:13.182+0100] {spark_submit.py:571} INFO - |1926|           1|
[2024-02-04T09:15:13.182+0100] {spark_submit.py:571} INFO - |1927|           2|
[2024-02-04T09:15:13.182+0100] {spark_submit.py:571} INFO - |1928|           2|
[2024-02-04T09:15:13.182+0100] {spark_submit.py:571} INFO - |1930|           1|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1931|           6|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1932|           4|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1933|           4|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1934|           1|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1935|           3|
[2024-02-04T09:15:13.183+0100] {spark_submit.py:571} INFO - |1936|           3|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1937|           1|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1938|           2|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1939|          10|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1940|           9|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1941|           7|
[2024-02-04T09:15:13.184+0100] {spark_submit.py:571} INFO - |1942|           9|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1943|           2|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1944|           5|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1945|           7|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1946|          10|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1947|           8|
[2024-02-04T09:15:13.185+0100] {spark_submit.py:571} INFO - |1948|          10|
[2024-02-04T09:15:13.186+0100] {spark_submit.py:571} INFO - |1949|           4|
[2024-02-04T09:15:13.186+0100] {spark_submit.py:571} INFO - |1950|           7|
[2024-02-04T09:15:13.202+0100] {spark_submit.py:571} INFO - |1951|           9|
[2024-02-04T09:15:13.209+0100] {spark_submit.py:571} INFO - |1952|           4|
[2024-02-04T09:15:13.209+0100] {spark_submit.py:571} INFO - |1953|          16|
[2024-02-04T09:15:13.210+0100] {spark_submit.py:571} INFO - |1954|          15|
[2024-02-04T09:15:13.210+0100] {spark_submit.py:571} INFO - |1955|          10|
[2024-02-04T09:15:13.210+0100] {spark_submit.py:571} INFO - |1956|          17|
[2024-02-04T09:15:13.210+0100] {spark_submit.py:571} INFO - |1957|          14|
[2024-02-04T09:15:13.211+0100] {spark_submit.py:571} INFO - |1958|          11|
[2024-02-04T09:15:13.211+0100] {spark_submit.py:571} INFO - |1959|          18|
[2024-02-04T09:15:13.211+0100] {spark_submit.py:571} INFO - |1960|          20|
[2024-02-04T09:15:13.211+0100] {spark_submit.py:571} INFO - |1961|          18|
[2024-02-04T09:15:13.212+0100] {spark_submit.py:571} INFO - |1962|          20|
[2024-02-04T09:15:13.212+0100] {spark_submit.py:571} INFO - |1963|          22|
[2024-02-04T09:15:13.212+0100] {spark_submit.py:571} INFO - |1964|          22|
[2024-02-04T09:15:13.213+0100] {spark_submit.py:571} INFO - |1965|          20|
[2024-02-04T09:15:13.213+0100] {spark_submit.py:571} INFO - |1966|          19|
[2024-02-04T09:15:13.213+0100] {spark_submit.py:571} INFO - |1967|          26|
[2024-02-04T09:15:13.214+0100] {spark_submit.py:571} INFO - |1968|          22|
[2024-02-04T09:15:13.214+0100] {spark_submit.py:571} INFO - |1969|          22|
[2024-02-04T09:15:13.215+0100] {spark_submit.py:571} INFO - |1970|          24|
[2024-02-04T09:15:13.215+0100] {spark_submit.py:571} INFO - |1971|          32|
[2024-02-04T09:15:13.215+0100] {spark_submit.py:571} INFO - |1972|          23|
[2024-02-04T09:15:13.216+0100] {spark_submit.py:571} INFO - |1973|          43|
[2024-02-04T09:15:13.216+0100] {spark_submit.py:571} INFO - |1974|          34|
[2024-02-04T09:15:13.216+0100] {spark_submit.py:571} INFO - |1975|          30|
[2024-02-04T09:15:13.217+0100] {spark_submit.py:571} INFO - |1976|          33|
[2024-02-04T09:15:13.217+0100] {spark_submit.py:571} INFO - |1977|          37|
[2024-02-04T09:15:13.217+0100] {spark_submit.py:571} INFO - |1978|          37|
[2024-02-04T09:15:13.217+0100] {spark_submit.py:571} INFO - |1979|          38|
[2024-02-04T09:15:13.245+0100] {spark_submit.py:571} INFO - |1980|          50|
[2024-02-04T09:15:13.246+0100] {spark_submit.py:571} INFO - |1981|          55|
[2024-02-04T09:15:13.246+0100] {spark_submit.py:571} INFO - |1982|          40|
[2024-02-04T09:15:13.246+0100] {spark_submit.py:571} INFO - |1983|          46|
[2024-02-04T09:15:13.247+0100] {spark_submit.py:571} INFO - |1984|          55|
[2024-02-04T09:15:13.247+0100] {spark_submit.py:571} INFO - |1985|          78|
[2024-02-04T09:15:13.247+0100] {spark_submit.py:571} INFO - |1986|          74|
[2024-02-04T09:15:13.247+0100] {spark_submit.py:571} INFO - |1987|          66|
[2024-02-04T09:15:13.248+0100] {spark_submit.py:571} INFO - |1988|          71|
[2024-02-04T09:15:13.248+0100] {spark_submit.py:571} INFO - |1989|          72|
[2024-02-04T09:15:13.248+0100] {spark_submit.py:571} INFO - |1990|          67|
[2024-02-04T09:15:13.249+0100] {spark_submit.py:571} INFO - |1991|          75|
[2024-02-04T09:15:13.249+0100] {spark_submit.py:571} INFO - |1992|          98|
[2024-02-04T09:15:13.249+0100] {spark_submit.py:571} INFO - |1993|         112|
[2024-02-04T09:15:13.253+0100] {spark_submit.py:571} INFO - |1994|          98|
[2024-02-04T09:15:13.254+0100] {spark_submit.py:571} INFO - |1995|         115|
[2024-02-04T09:15:13.254+0100] {spark_submit.py:571} INFO - |1996|         102|
[2024-02-04T09:15:13.255+0100] {spark_submit.py:571} INFO - |1997|         112|
[2024-02-04T09:15:13.255+0100] {spark_submit.py:571} INFO - |1998|         135|
[2024-02-04T09:15:13.255+0100] {spark_submit.py:571} INFO - |1999|         130|
[2024-02-04T09:15:13.256+0100] {spark_submit.py:571} INFO - |2000|         124|
[2024-02-04T09:15:13.256+0100] {spark_submit.py:571} INFO - |2001|         144|
[2024-02-04T09:15:13.256+0100] {spark_submit.py:571} INFO - |2002|         159|
[2024-02-04T09:15:13.256+0100] {spark_submit.py:571} INFO - |2003|         166|
[2024-02-04T09:15:13.256+0100] {spark_submit.py:571} INFO - |2004|         189|
[2024-02-04T09:15:13.257+0100] {spark_submit.py:571} INFO - |2005|         178|
[2024-02-04T09:15:13.257+0100] {spark_submit.py:571} INFO - |2006|         206|
[2024-02-04T09:15:13.257+0100] {spark_submit.py:571} INFO - |2007|         211|
[2024-02-04T09:15:13.257+0100] {spark_submit.py:571} INFO - |2008|         210|
[2024-02-04T09:15:13.258+0100] {spark_submit.py:571} INFO - |2009|         233|
[2024-02-04T09:15:13.258+0100] {spark_submit.py:571} INFO - |2010|         223|
[2024-02-04T09:15:13.258+0100] {spark_submit.py:571} INFO - |2011|         258|
[2024-02-04T09:15:13.258+0100] {spark_submit.py:571} INFO - |2012|         263|
[2024-02-04T09:15:13.258+0100] {spark_submit.py:571} INFO - |2013|         296|
[2024-02-04T09:15:13.259+0100] {spark_submit.py:571} INFO - |2014|         313|
[2024-02-04T09:15:13.259+0100] {spark_submit.py:571} INFO - |2015|         335|
[2024-02-04T09:15:13.259+0100] {spark_submit.py:571} INFO - |2016|         363|
[2024-02-04T09:15:13.259+0100] {spark_submit.py:571} INFO - |2017|         393|
[2024-02-04T09:15:13.259+0100] {spark_submit.py:571} INFO - |2018|         433|
[2024-02-04T09:15:13.260+0100] {spark_submit.py:571} INFO - |2019|         450|
[2024-02-04T09:15:13.260+0100] {spark_submit.py:571} INFO - +----+------------+
[2024-02-04T09:15:13.260+0100] {spark_submit.py:571} INFO - only showing top 100 rows
[2024-02-04T09:15:13.260+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:14.083+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO FileSourceStrategy: Pushed Filters:
[2024-02-04T09:15:14.085+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(year(cast(gettimestamp(release_date#9, yyyy-MM-dd, TimestampType, Some(Europe/Paris), false) as date)))
[2024-02-04T09:15:14.093+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO FileSourceStrategy: Output Data Schema: struct<original_language: string, popularity: double, release_date: string, vote_count: bigint ... 2 more fields>
[2024-02-04T09:15:14.308+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO CodeGenerator: Code generated in 80.276209 ms
[2024-02-04T09:15:14.318+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.4 KiB, free 433.7 MiB)
[2024-02-04T09:15:14.343+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.7 MiB)
[2024-02-04T09:15:14.345+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.2.15:33381 (size: 34.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:14.348+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO SparkContext: Created broadcast 10 from show at MnMcount.scala:66
[2024-02-04T09:15:14.354+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-04T09:15:14.370+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Registering RDD 26 (show at MnMcount.scala:66) as input to shuffle 3
[2024-02-04T09:15:14.371+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Got map stage job 7 (show at MnMcount.scala:66) with 1 output partitions
[2024-02-04T09:15:14.372+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (show at MnMcount.scala:66)
[2024-02-04T09:15:14.373+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Parents of final stage: List()
[2024-02-04T09:15:14.375+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:14.378+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[26] at show at MnMcount.scala:66), which has no missing parents
[2024-02-04T09:15:14.396+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 19.2 KiB, free 433.6 MiB)
[2024-02-04T09:15:14.417+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.6 MiB)
[2024-02-04T09:15:14.420+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.2.15:33381 (size: 9.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:14.428+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:33381 in memory (size: 34.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:14.430+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:14.431+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[26] at show at MnMcount.scala:66) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:14.432+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-02-04T09:15:14.437+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 7) (10.0.2.15, executor driver, partition 0, ANY, 4943 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:14.441+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO Executor: Running task 0.0 in stage 12.0 (TID 7)
[2024-02-04T09:15:14.492+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.2.15:33381 in memory (size: 19.6 KiB, free: 434.3 MiB)
[2024-02-04T09:15:14.509+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/user/project/datalake/raw/2024-02-04/tmdb_popular_movies.json, range: 0-6636236, partition values: [empty row]
[2024-02-04T09:15:14.561+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:14 INFO CodeGenerator: Code generated in 34.356982 ms
[2024-02-04T09:15:15.521+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 7). 2038 bytes result sent to driver
[2024-02-04T09:15:15.527+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 7) in 1090 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:15.528+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-02-04T09:15:15.529+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: ShuffleMapStage 12 (show at MnMcount.scala:66) finished in 1.148 s
[2024-02-04T09:15:15.530+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: looking for newly runnable stages
[2024-02-04T09:15:15.530+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: running: Set()
[2024-02-04T09:15:15.530+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: waiting: Set()
[2024-02-04T09:15:15.530+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: failed: Set()
[2024-02-04T09:15:15.556+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-02-04T09:15:15.625+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO CodeGenerator: Code generated in 18.981176 ms
[2024-02-04T09:15:15.663+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO CodeGenerator: Code generated in 18.271629 ms
[2024-02-04T09:15:15.728+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO SparkContext: Starting job: show at MnMcount.scala:66
[2024-02-04T09:15:15.733+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Got job 8 (show at MnMcount.scala:66) with 1 output partitions
[2024-02-04T09:15:15.759+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Final stage: ResultStage 14 (show at MnMcount.scala:66)
[2024-02-04T09:15:15.760+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
[2024-02-04T09:15:15.761+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:15.761+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[31] at show at MnMcount.scala:66), which has no missing parents
[2024-02-04T09:15:15.840+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 38.5 KiB, free 433.9 MiB)
[2024-02-04T09:15:15.845+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 433.9 MiB)
[2024-02-04T09:15:15.846+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.2.15:33381 (size: 18.4 KiB, free: 434.3 MiB)
[2024-02-04T09:15:15.853+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:15.856+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[31] at show at MnMcount.scala:66) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:15.857+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-02-04T09:15:15.860+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 8) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:15.864+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO Executor: Running task 0.0 in stage 14.0 (TID 8)
[2024-02-04T09:15:15.940+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO ShuffleBlockFetcherIterator: Getting 1 (146.4 KiB) non-empty blocks including 1 (146.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:15.941+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-02-04T09:15:15.965+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO CodeGenerator: Code generated in 11.318321 ms
[2024-02-04T09:15:15.998+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:15 INFO CodeGenerator: Code generated in 12.116654 ms
[2024-02-04T09:15:16.125+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO CodeGenerator: Code generated in 8.222369 ms
[2024-02-04T09:15:16.144+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO CodeGenerator: Code generated in 10.797618 ms
[2024-02-04T09:15:16.161+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO CodeGenerator: Code generated in 11.677124 ms
[2024-02-04T09:15:16.254+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO Executor: Finished task 0.0 in stage 14.0 (TID 8). 5830 bytes result sent to driver
[2024-02-04T09:15:16.261+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 8) in 400 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:16.262+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-02-04T09:15:16.263+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.2.15:33381 in memory (size: 9.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:16.272+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: ResultStage 14 (show at MnMcount.scala:66) finished in 0.522 s
[2024-02-04T09:15:16.274+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:15:16.283+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-02-04T09:15:16.284+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Job 8 finished: show at MnMcount.scala:66, took 0.546033 s
[2024-02-04T09:15:16.378+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO CodeGenerator: Code generated in 39.113375 ms
[2024-02-04T09:15:16.404+0100] {spark_submit.py:571} INFO - +----+-----------------+----------+----------+
[2024-02-04T09:15:16.405+0100] {spark_submit.py:571} INFO - |year|original_language|vote_count|popularity|
[2024-02-04T09:15:16.406+0100] {spark_submit.py:571} INFO - +----+-----------------+----------+----------+
[2024-02-04T09:15:16.407+0100] {spark_submit.py:571} INFO - |1902|               fr|      1652|    15.932|
[2024-02-04T09:15:16.408+0100] {spark_submit.py:571} INFO - |1920|               de|      1449|    19.576|
[2024-02-04T09:15:16.409+0100] {spark_submit.py:571} INFO - |1921|               en|      1943|    16.531|
[2024-02-04T09:15:16.410+0100] {spark_submit.py:571} INFO - |1922|               de|      1835|    21.973|
[2024-02-04T09:15:16.411+0100] {spark_submit.py:571} INFO - |1923|               en|       456|    13.448|
[2024-02-04T09:15:16.413+0100] {spark_submit.py:571} INFO - |1924|               en|       897|    11.579|
[2024-02-04T09:15:16.414+0100] {spark_submit.py:571} INFO - |1925|               en|      1482|    12.995|
[2024-02-04T09:15:16.414+0100] {spark_submit.py:571} INFO - |1926|               en|      1133|    15.893|
[2024-02-04T09:15:16.415+0100] {spark_submit.py:571} INFO - |1927|               de|      2524|    26.326|
[2024-02-04T09:15:16.416+0100] {spark_submit.py:571} INFO - |1928|               en|       494|    15.211|
[2024-02-04T09:15:16.417+0100] {spark_submit.py:571} INFO - |1930|               en|       731|    25.763|
[2024-02-04T09:15:16.418+0100] {spark_submit.py:571} INFO - |1931|               en|      2022|    20.019|
[2024-02-04T09:15:16.419+0100] {spark_submit.py:571} INFO - |1932|               en|      1098|    15.959|
[2024-02-04T09:15:16.420+0100] {spark_submit.py:571} INFO - |1933|               en|      1367|    23.002|
[2024-02-04T09:15:16.421+0100] {spark_submit.py:571} INFO - |1934|               en|       360|    11.556|
[2024-02-04T09:15:16.421+0100] {spark_submit.py:571} INFO - |1935|               en|       882|    13.175|
[2024-02-04T09:15:16.429+0100] {spark_submit.py:571} INFO - |1936|               en|      3490|    15.731|
[2024-02-04T09:15:16.434+0100] {spark_submit.py:571} INFO - |1937|               en|      7015|    58.956|
[2024-02-04T09:15:16.436+0100] {spark_submit.py:571} INFO - |1938|               en|       903|    12.049|
[2024-02-04T09:15:16.438+0100] {spark_submit.py:571} INFO - |1939|               en|      5234|    36.585|
[2024-02-04T09:15:16.439+0100] {spark_submit.py:571} INFO - |1940|               en|      5565|    37.479|
[2024-02-04T09:15:16.440+0100] {spark_submit.py:571} INFO - |1941|               en|      5184|    28.391|
[2024-02-04T09:15:16.441+0100] {spark_submit.py:571} INFO - |1942|               en|      5354|    38.134|
[2024-02-04T09:15:16.442+0100] {spark_submit.py:571} INFO - |1943|               en|       953|    15.001|
[2024-02-04T09:15:16.442+0100] {spark_submit.py:571} INFO - |1944|               en|      1674|    17.485|
[2024-02-04T09:15:16.443+0100] {spark_submit.py:571} INFO - |1945|               it|       816|    14.732|
[2024-02-04T09:15:16.444+0100] {spark_submit.py:571} INFO - |1946|               en|      4028|    39.474|
[2024-02-04T09:15:16.445+0100] {spark_submit.py:571} INFO - |1947|               en|       683|    24.324|
[2024-02-04T09:15:16.446+0100] {spark_submit.py:571} INFO - |1948|               en|      2484|    19.428|
[2024-02-04T09:15:16.447+0100] {spark_submit.py:571} INFO - |1949|               en|       546|     19.06|
[2024-02-04T09:15:16.448+0100] {spark_submit.py:571} INFO - |1950|               en|      6387|    78.017|
[2024-02-04T09:15:16.449+0100] {spark_submit.py:571} INFO - |1951|               en|      5578|    51.889|
[2024-02-04T09:15:16.450+0100] {spark_submit.py:571} INFO - |1952|               en|      2934|    25.864|
[2024-02-04T09:15:16.450+0100] {spark_submit.py:571} INFO - |1953|               en|      5112|    48.025|
[2024-02-04T09:15:16.452+0100] {spark_submit.py:571} INFO - |1954|               en|      6110|     28.93|
[2024-02-04T09:15:16.453+0100] {spark_submit.py:571} INFO - |1955|               en|      4993|    36.618|
[2024-02-04T09:15:16.454+0100] {spark_submit.py:571} INFO - |1956|               en|      1456|    36.875|
[2024-02-04T09:15:16.454+0100] {spark_submit.py:571} INFO - |1957|               en|      7996|     43.52|
[2024-02-04T09:15:16.455+0100] {spark_submit.py:571} INFO - |1958|               en|      5397|    30.823|
[2024-02-04T09:15:16.456+0100] {spark_submit.py:571} INFO - |1959|               en|      4892|    38.325|
[2024-02-04T09:15:16.457+0100] {spark_submit.py:571} INFO - |1960|               en|      9555|    42.274|
[2024-02-04T09:15:16.458+0100] {spark_submit.py:571} INFO - |1961|               en|      5968|    45.726|
[2024-02-04T09:15:16.459+0100] {spark_submit.py:571} INFO - |1962|               en|      3379|    31.906|
[2024-02-04T09:15:16.460+0100] {spark_submit.py:571} INFO - |1963|               en|      3820|    24.181|
[2024-02-04T09:15:16.461+0100] {spark_submit.py:571} INFO - |1964|               en|      5262|    24.853|
[2024-02-04T09:15:16.461+0100] {spark_submit.py:571} INFO - |1965|               it|      3682|    32.229|
[2024-02-04T09:15:16.462+0100] {spark_submit.py:571} INFO - |1966|               it|      8054|    53.254|
[2024-02-04T09:15:16.463+0100] {spark_submit.py:571} INFO - |1967|               en|      5941|    51.526|
[2024-02-04T09:15:16.464+0100] {spark_submit.py:571} INFO - |1968|               en|     10872|    48.882|
[2024-02-04T09:15:16.465+0100] {spark_submit.py:571} INFO - |1969|               en|      2040|     20.18|
[2024-02-04T09:15:16.466+0100] {spark_submit.py:571} INFO - |1970|               en|      4767|    33.628|
[2024-02-04T09:15:16.468+0100] {spark_submit.py:571} INFO - |1971|               en|     12257|    38.979|
[2024-02-04T09:15:16.469+0100] {spark_submit.py:571} INFO - |1972|               en|     19386|   103.574|
[2024-02-04T09:15:16.470+0100] {spark_submit.py:571} INFO - |1973|               en|      7547|    46.842|
[2024-02-04T09:15:16.471+0100] {spark_submit.py:571} INFO - |1974|               en|     11697|    68.574|
[2024-02-04T09:15:16.472+0100] {spark_submit.py:571} INFO - |1975|               en|      9910|    31.912|
[2024-02-04T09:15:16.473+0100] {spark_submit.py:571} INFO - |1976|               en|     11485|    42.845|
[2024-02-04T09:15:16.474+0100] {spark_submit.py:571} INFO - |1977|               en|     19621|    92.727|
[2024-02-04T09:15:16.474+0100] {spark_submit.py:571} INFO - |1978|               en|      6884|    45.753|
[2024-02-04T09:15:16.476+0100] {spark_submit.py:571} INFO - |1979|               en|     13609|    65.503|
[2024-02-04T09:15:16.476+0100] {spark_submit.py:571} INFO - |1980|               en|     16626|    49.062|
[2024-02-04T09:15:16.478+0100] {spark_submit.py:571} INFO - |1981|               en|     11815|    45.902|
[2024-02-04T09:15:16.479+0100] {spark_submit.py:571} INFO - |1982|               en|     13119|    47.819|
[2024-02-04T09:15:16.480+0100] {spark_submit.py:571} INFO - |1983|               en|     11110|     56.48|
[2024-02-04T09:15:16.481+0100] {spark_submit.py:571} INFO - |1984|               en|     12214|    59.063|
[2024-02-04T09:15:16.481+0100] {spark_submit.py:571} INFO - |1985|               en|     18928|    69.946|
[2024-02-04T09:15:16.483+0100] {spark_submit.py:571} INFO - |1986|               en|      9007|    60.908|
[2024-02-04T09:15:16.483+0100] {spark_submit.py:571} INFO - |1987|               en|      9928|    30.408|
[2024-02-04T09:15:16.484+0100] {spark_submit.py:571} INFO - |1988|               en|     10560|    42.936|
[2024-02-04T09:15:16.485+0100] {spark_submit.py:571} INFO - |1989|               en|     12133|    29.605|
[2024-02-04T09:15:16.486+0100] {spark_submit.py:571} INFO - |1990|               en|     12326|    39.764|
[2024-02-04T09:15:16.487+0100] {spark_submit.py:571} INFO - |1991|               en|     12102|    68.882|
[2024-02-04T09:15:16.488+0100] {spark_submit.py:571} INFO - |1992|               en|     13632|    42.811|
[2024-02-04T09:15:16.489+0100] {spark_submit.py:571} INFO - |1993|               en|     15501|    29.965|
[2024-02-04T09:15:16.490+0100] {spark_submit.py:571} INFO - |1994|               en|     26576|    80.544|
[2024-02-04T09:15:16.491+0100] {spark_submit.py:571} INFO - |1995|               en|     19945|    99.352|
[2024-02-04T09:15:16.492+0100] {spark_submit.py:571} INFO - |1996|               en|      9205|    28.744|
[2024-02-04T09:15:16.493+0100] {spark_submit.py:571} INFO - |1997|               en|     24203|   118.666|
[2024-02-04T09:15:16.494+0100] {spark_submit.py:571} INFO - |1998|               en|     17336|    60.766|
[2024-02-04T09:15:16.495+0100] {spark_submit.py:571} INFO - |1999|               en|     27972|    77.543|
[2024-02-04T09:15:16.496+0100] {spark_submit.py:571} INFO - |2000|               en|     17482|    55.322|
[2024-02-04T09:15:16.497+0100] {spark_submit.py:571} INFO - |2001|               en|     26051|   177.068|
[2024-02-04T09:15:16.498+0100] {spark_submit.py:571} INFO - |2002|               en|     20935|   149.767|
[2024-02-04T09:15:16.499+0100] {spark_submit.py:571} INFO - |2003|               en|     22965|     99.97|
[2024-02-04T09:15:16.500+0100] {spark_submit.py:571} INFO - |2004|               en|     20539|   153.372|
[2024-02-04T09:15:16.501+0100] {spark_submit.py:571} INFO - |2005|               en|     20036|    52.746|
[2024-02-04T09:15:16.501+0100] {spark_submit.py:571} INFO - |2006|               en|     15180|   104.355|
[2024-02-04T09:15:16.502+0100] {spark_submit.py:571} INFO - |2007|               en|     18620|   140.636|
[2024-02-04T09:15:16.503+0100] {spark_submit.py:571} INFO - |2008|               en|     31382|    88.854|
[2024-02-04T09:15:16.504+0100] {spark_submit.py:571} INFO - |2009|               en|     30412|   109.621|
[2024-02-04T09:15:16.505+0100] {spark_submit.py:571} INFO - |2010|               en|     35186|    85.871|
[2024-02-04T09:15:16.506+0100] {spark_submit.py:571} INFO - |2011|               en|     20632|    55.415|
[2024-02-04T09:15:16.507+0100] {spark_submit.py:571} INFO - |2012|               en|     25201|    58.186|
[2024-02-04T09:15:16.507+0100] {spark_submit.py:571} INFO - |2013|               en|     22827|    90.785|
[2024-02-04T09:15:16.509+0100] {spark_submit.py:571} INFO - |2014|               en|     33532|   149.083|
[2024-02-04T09:15:16.510+0100] {spark_submit.py:571} INFO - |2015|               en|     19967|   133.473|
[2024-02-04T09:15:16.510+0100] {spark_submit.py:571} INFO - |2016|               en|     29384|   127.161|
[2024-02-04T09:15:16.511+0100] {spark_submit.py:571} INFO - |2017|               en|     20735|    65.205|
[2024-02-04T09:15:16.512+0100] {spark_submit.py:571} INFO - |2018|               en|     28379|   198.645|
[2024-02-04T09:15:16.513+0100] {spark_submit.py:571} INFO - |2019|               en|     24032|    73.382|
[2024-02-04T09:15:16.514+0100] {spark_submit.py:571} INFO - +----+-----------------+----------+----------+
[2024-02-04T09:15:16.515+0100] {spark_submit.py:571} INFO - only showing top 100 rows
[2024-02-04T09:15:16.516+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:16.517+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileSourceStrategy: Pushed Filters:
[2024-02-04T09:15:16.518+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(year(cast(gettimestamp(release_date#9, yyyy-MM-dd, TimestampType, Some(Europe/Paris), false) as date)))
[2024-02-04T09:15:16.519+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileSourceStrategy: Output Data Schema: struct<original_language: string, popularity: double, release_date: string, vote_count: bigint ... 2 more fields>
[2024-02-04T09:15:16.553+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:16.554+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:16.555+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:16.556+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:16.557+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:16.565+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:16.568+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:16.634+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 199.4 KiB, free 433.7 MiB)
[2024-02-04T09:15:16.653+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.7 MiB)
[2024-02-04T09:15:16.658+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.2.15:33381 (size: 34.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:16.661+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO SparkContext: Created broadcast 13 from parquet at MnMcount.scala:69
[2024-02-04T09:15:16.664+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-02-04T09:15:16.680+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Registering RDD 35 (parquet at MnMcount.scala:69) as input to shuffle 4
[2024-02-04T09:15:16.681+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Got map stage job 9 (parquet at MnMcount.scala:69) with 1 output partitions
[2024-02-04T09:15:16.682+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (parquet at MnMcount.scala:69)
[2024-02-04T09:15:16.682+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Parents of final stage: List()
[2024-02-04T09:15:16.683+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:16.687+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[35] at parquet at MnMcount.scala:69), which has no missing parents
[2024-02-04T09:15:16.713+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 19.2 KiB, free 433.6 MiB)
[2024-02-04T09:15:16.716+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 433.6 MiB)
[2024-02-04T09:15:16.717+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.2.15:33381 (size: 9.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:16.719+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:16.720+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[35] at parquet at MnMcount.scala:69) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:16.722+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-02-04T09:15:16.726+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 9) (10.0.2.15, executor driver, partition 0, ANY, 4943 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:16.729+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO Executor: Running task 0.0 in stage 15.0 (TID 9)
[2024-02-04T09:15:16.755+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:16 INFO FileScanRDD: Reading File path: hdfs://localhost:9000/user/project/datalake/raw/2024-02-04/tmdb_popular_movies.json, range: 0-6636236, partition values: [empty row]
[2024-02-04T09:15:17.257+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO Executor: Finished task 0.0 in stage 15.0 (TID 9). 2038 bytes result sent to driver
[2024-02-04T09:15:17.274+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 9) in 548 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:17.278+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-02-04T09:15:17.280+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: ShuffleMapStage 15 (parquet at MnMcount.scala:69) finished in 0.584 s
[2024-02-04T09:15:17.281+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: looking for newly runnable stages
[2024-02-04T09:15:17.282+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: running: Set()
[2024-02-04T09:15:17.284+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: waiting: Set()
[2024-02-04T09:15:17.285+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: failed: Set()
[2024-02-04T09:15:17.291+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-02-04T09:15:17.363+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO CodeGenerator: Code generated in 21.690802 ms
[2024-02-04T09:15:17.412+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO SparkContext: Starting job: parquet at MnMcount.scala:69
[2024-02-04T09:15:17.419+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Got job 10 (parquet at MnMcount.scala:69) with 1 output partitions
[2024-02-04T09:15:17.422+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Final stage: ResultStage 17 (parquet at MnMcount.scala:69)
[2024-02-04T09:15:17.437+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2024-02-04T09:15:17.438+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Missing parents: List()
[2024-02-04T09:15:17.443+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[39] at parquet at MnMcount.scala:69), which has no missing parents
[2024-02-04T09:15:17.484+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 238.0 KiB, free 433.4 MiB)
[2024-02-04T09:15:17.511+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 88.4 KiB, free 433.3 MiB)
[2024-02-04T09:15:17.512+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.2.15:33381 (size: 88.4 KiB, free: 434.2 MiB)
[2024-02-04T09:15:17.515+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1509
[2024-02-04T09:15:17.515+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[39] at parquet at MnMcount.scala:69) (first 15 tasks are for partitions Vector(0))
[2024-02-04T09:15:17.516+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-02-04T09:15:17.520+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 10) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[2024-02-04T09:15:17.524+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO Executor: Running task 0.0 in stage 17.0 (TID 10)
[2024-02-04T09:15:17.544+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.2.15:33381 in memory (size: 9.1 KiB, free: 434.2 MiB)
[2024-02-04T09:15:17.573+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ShuffleBlockFetcherIterator: Getting 1 (146.4 KiB) non-empty blocks including 1 (146.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-02-04T09:15:17.574+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2024-02-04T09:15:17.601+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.2.15:33381 in memory (size: 34.1 KiB, free: 434.2 MiB)
[2024-02-04T09:15:17.657+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.2.15:33381 in memory (size: 18.4 KiB, free: 434.2 MiB)
[2024-02-04T09:15:17.671+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:17.672+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:17.674+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:17.674+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-02-04T09:15:17.675+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-02-04T09:15:17.682+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-02-04T09:15:17.683+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO CodecConfig: Compression: SNAPPY
[2024-02-04T09:15:17.697+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO CodecConfig: Compression: SNAPPY
[2024-02-04T09:15:17.703+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ParquetOutputFormat: Parquet block size to 134217728
[2024-02-04T09:15:17.704+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ParquetOutputFormat: Validation is off
[2024-02-04T09:15:17.704+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2024-02-04T09:15:17.704+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ParquetOutputFormat: Parquet properties are:
[2024-02-04T09:15:17.705+0100] {spark_submit.py:571} INFO - Parquet page size to 1048576
[2024-02-04T09:15:17.705+0100] {spark_submit.py:571} INFO - Parquet dictionary page size to 1048576
[2024-02-04T09:15:17.705+0100] {spark_submit.py:571} INFO - Dictionary is true
[2024-02-04T09:15:17.705+0100] {spark_submit.py:571} INFO - Writer version is: PARQUET_1_0
[2024-02-04T09:15:17.705+0100] {spark_submit.py:571} INFO - Page size checking is: estimated
[2024-02-04T09:15:17.706+0100] {spark_submit.py:571} INFO - Min row count for page size check is: 100
[2024-02-04T09:15:17.706+0100] {spark_submit.py:571} INFO - Max row count for page size check is: 10000
[2024-02-04T09:15:17.706+0100] {spark_submit.py:571} INFO - Truncate length for column indexes is: 64
[2024-02-04T09:15:17.706+0100] {spark_submit.py:571} INFO - Truncate length for statistics min/max  is: 2147483647
[2024-02-04T09:15:17.707+0100] {spark_submit.py:571} INFO - Bloom filter enabled: false
[2024-02-04T09:15:17.707+0100] {spark_submit.py:571} INFO - Max Bloom filter size for a column is 1048576
[2024-02-04T09:15:17.707+0100] {spark_submit.py:571} INFO - Bloom filter expected number of distinct values are: null
[2024-02-04T09:15:17.707+0100] {spark_submit.py:571} INFO - Page row count limit to 20000
[2024-02-04T09:15:17.708+0100] {spark_submit.py:571} INFO - Writing page checksums is: on
[2024-02-04T09:15:17.796+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-02-04T09:15:17.797+0100] {spark_submit.py:571} INFO - {
[2024-02-04T09:15:17.797+0100] {spark_submit.py:571} INFO - "type" : "struct",
[2024-02-04T09:15:17.798+0100] {spark_submit.py:571} INFO - "fields" : [ {
[2024-02-04T09:15:17.798+0100] {spark_submit.py:571} INFO - "name" : "year",
[2024-02-04T09:15:17.799+0100] {spark_submit.py:571} INFO - "type" : "integer",
[2024-02-04T09:15:17.799+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-04T09:15:17.799+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:17.799+0100] {spark_submit.py:571} INFO - }, {
[2024-02-04T09:15:17.800+0100] {spark_submit.py:571} INFO - "name" : "original_language",
[2024-02-04T09:15:17.800+0100] {spark_submit.py:571} INFO - "type" : "string",
[2024-02-04T09:15:17.800+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-04T09:15:17.800+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:17.801+0100] {spark_submit.py:571} INFO - }, {
[2024-02-04T09:15:17.801+0100] {spark_submit.py:571} INFO - "name" : "vote_count",
[2024-02-04T09:15:17.801+0100] {spark_submit.py:571} INFO - "type" : "long",
[2024-02-04T09:15:17.802+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-04T09:15:17.802+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:17.802+0100] {spark_submit.py:571} INFO - }, {
[2024-02-04T09:15:17.802+0100] {spark_submit.py:571} INFO - "name" : "popularity",
[2024-02-04T09:15:17.802+0100] {spark_submit.py:571} INFO - "type" : "double",
[2024-02-04T09:15:17.803+0100] {spark_submit.py:571} INFO - "nullable" : true,
[2024-02-04T09:15:17.803+0100] {spark_submit.py:571} INFO - "metadata" : { }
[2024-02-04T09:15:17.803+0100] {spark_submit.py:571} INFO - } ]
[2024-02-04T09:15:17.803+0100] {spark_submit.py:571} INFO - }
[2024-02-04T09:15:17.803+0100] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - message spark_schema {
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - optional int32 year;
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - optional binary original_language (STRING);
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - optional int64 vote_count;
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - optional double popularity;
[2024-02-04T09:15:17.804+0100] {spark_submit.py:571} INFO - }
[2024-02-04T09:15:17.805+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:17.805+0100] {spark_submit.py:571} INFO - 
[2024-02-04T09:15:17.863+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:33381 in memory (size: 34.1 KiB, free: 434.3 MiB)
[2024-02-04T09:15:18.704+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO FileOutputCommitter: Saved output of task 'attempt_202402040915178867597314663423673_0017_m_000000_10' to hdfs://localhost:9000/user/project/datalake/usage/2024-02-04/movies_language/movies_language.parquet/_temporary/0/task_202402040915178867597314663423673_0017_m_000000
[2024-02-04T09:15:18.705+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO SparkHadoopMapRedUtil: attempt_202402040915178867597314663423673_0017_m_000000_10: Committed. Elapsed time: 37 ms.
[2024-02-04T09:15:18.714+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO Executor: Finished task 0.0 in stage 17.0 (TID 10). 4385 bytes result sent to driver
[2024-02-04T09:15:18.720+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 10) in 1201 ms on 10.0.2.15 (executor driver) (1/1)
[2024-02-04T09:15:18.721+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO DAGScheduler: ResultStage 17 (parquet at MnMcount.scala:69) finished in 1.290 s
[2024-02-04T09:15:18.722+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-04T09:15:18.723+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-02-04T09:15:18.724+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-02-04T09:15:18.725+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO DAGScheduler: Job 10 finished: parquet at MnMcount.scala:69, took 1.311595 s
[2024-02-04T09:15:18.728+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO FileFormatWriter: Start to commit write Job 5379e67e-71f6-4d7c-998d-8bca2646a1f7.
[2024-02-04T09:15:18.864+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO FileFormatWriter: Write Job 5379e67e-71f6-4d7c-998d-8bca2646a1f7 committed. Elapsed time: 136 ms.
[2024-02-04T09:15:18.865+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO FileFormatWriter: Finished processing stats for write job 5379e67e-71f6-4d7c-998d-8bca2646a1f7.
[2024-02-04T09:15:18.921+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO SparkContext: Invoking stop() from shutdown hook
[2024-02-04T09:15:18.980+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:18 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4042
[2024-02-04T09:15:19.051+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-02-04T09:15:19.149+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO MemoryStore: MemoryStore cleared
[2024-02-04T09:15:19.154+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO BlockManager: BlockManager stopped
[2024-02-04T09:15:19.175+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-02-04T09:15:19.222+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-02-04T09:15:19.372+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO SparkContext: Successfully stopped SparkContext
[2024-02-04T09:15:19.373+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO ShutdownHookManager: Shutdown hook called
[2024-02-04T09:15:19.375+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9d237d9-f321-41d6-b7b8-640d02ebf073
[2024-02-04T09:15:19.385+0100] {spark_submit.py:571} INFO - 24/02/04 09:15:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-accce164-802d-49ea-aa51-f4fe34771d1c
[2024-02-04T09:15:19.517+0100] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=jeniferhdfs, task_id=submit_spark_job, execution_date=20240203T000000, start_date=20240204T081354, end_date=20240204T081519
[2024-02-04T09:15:19.581+0100] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-02-04T09:15:19.593+0100] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
